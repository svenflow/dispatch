#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "torch",
#     "torchvision",
#     "pillow",
#     "numpy",
# ]
# ///
"""
CPPN Multi-Image Learning - Shared Latent Space

Train a single CPPN to learn multiple images, each with a unique latent code.
Then interpolate smoothly between images in latent space.

Usage:
    cppn-multi train <image1> <image2> <output_model> [--epochs 3000]
    cppn-multi morph <model> <output_video> [--duration 6]
"""

import argparse
import math
import subprocess
import tempfile
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image


class SIREN(nn.Module):
    """Sinusoidal Representation Network with learnable latent codes."""

    def __init__(self, hidden_features=256, hidden_layers=8, latent_dim=32, omega_0=30.0):
        super().__init__()
        self.latent_dim = latent_dim
        self.omega_0 = omega_0

        # Input: x, y, r (3) + latent (latent_dim)
        self.input_layer = nn.Linear(3 + latent_dim, hidden_features)

        self.hidden_layers = nn.ModuleList([
            nn.Linear(hidden_features, hidden_features)
            for _ in range(hidden_layers)
        ])

        self.output_layer = nn.Linear(hidden_features, 3)
        self._init_weights()

    def _init_weights(self):
        with torch.no_grad():
            self.input_layer.weight.uniform_(-1 / (3 + self.latent_dim),
                                              1 / (3 + self.latent_dim))
            for layer in self.hidden_layers:
                layer.weight.uniform_(-math.sqrt(6 / layer.in_features) / self.omega_0,
                                       math.sqrt(6 / layer.in_features) / self.omega_0)
            self.output_layer.weight.uniform_(-math.sqrt(6 / self.output_layer.in_features) / self.omega_0,
                                               math.sqrt(6 / self.output_layer.in_features) / self.omega_0)

    def forward(self, coords, latent):
        # coords: (N, 3) - x, y, r
        # latent: (latent_dim,) or (N, latent_dim)
        if latent.dim() == 1:
            latent = latent.unsqueeze(0).expand(coords.shape[0], -1)

        x = torch.cat([coords, latent], dim=-1)
        x = torch.sin(self.omega_0 * self.input_layer(x))

        for layer in self.hidden_layers:
            x = torch.sin(self.omega_0 * layer(x))

        x = self.output_layer(x)
        x = torch.sigmoid(x)
        return x


def create_coordinate_grid(width, height, device='cpu'):
    """Create normalized coordinate grid with x, y, r."""
    y_coords = torch.linspace(-1, 1, height, device=device)
    x_coords = torch.linspace(-1, 1, width, device=device)
    yy, xx = torch.meshgrid(y_coords, x_coords, indexing='ij')
    rr = torch.sqrt(xx**2 + yy**2)
    coords = torch.stack([xx, yy, rr], dim=-1)
    return coords.reshape(-1, 3)


def load_image(path, size=256):
    """Load and resize image to square."""
    img = Image.open(path).convert('RGB')
    img = img.resize((size, size), Image.LANCZOS)
    arr = np.array(img).astype(np.float32) / 255.0
    return torch.from_numpy(arr)


def train_multi(image1_path, image2_path, output_path, epochs=3000,
                hidden=256, layers=8, latent_dim=32, lr=1e-4, device='cpu'):
    """Train single CPPN on two images with separate latent codes."""

    print(f"Loading images...")
    size = 256
    img1 = load_image(image1_path, size).to(device)
    img2 = load_image(image2_path, size).to(device)
    target1 = img1.reshape(-1, 3)
    target2 = img2.reshape(-1, 3)

    print(f"Training shared CPPN on 2 images ({size}x{size})")
    print(f"Architecture: {hidden} hidden, {layers} layers, {latent_dim}D latent")

    # Create model
    model = SIREN(
        hidden_features=hidden,
        hidden_layers=layers,
        latent_dim=latent_dim
    ).to(device)

    # Learnable latent codes for each image
    latent1 = nn.Parameter(torch.randn(latent_dim, device=device) * 0.1)
    latent2 = nn.Parameter(torch.randn(latent_dim, device=device) * 0.1)

    # Coordinate grid
    coords = create_coordinate_grid(size, size, device)

    # Optimizer for both model and latents
    optimizer = optim.Adam([
        {'params': model.parameters(), 'lr': lr},
        {'params': [latent1, latent2], 'lr': lr * 10}  # Higher LR for latents
    ])
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)

    for epoch in range(epochs):
        optimizer.zero_grad()

        # Forward pass for both images
        out1 = model(coords, latent1)
        out2 = model(coords, latent2)

        loss1 = nn.functional.mse_loss(out1, target1)
        loss2 = nn.functional.mse_loss(out2, target2)

        # Also add a small regularization to keep latents from drifting too far
        reg = 0.001 * (latent1.norm() + latent2.norm())

        loss = loss1 + loss2 + reg

        loss.backward()
        optimizer.step()
        scheduler.step()

        if (epoch + 1) % 200 == 0 or epoch == 0:
            psnr1 = -10 * math.log10(loss1.item())
            psnr2 = -10 * math.log10(loss2.item())
            print(f"Epoch {epoch+1}/{epochs} | PSNR1: {psnr1:.1f} dB | PSNR2: {psnr2:.1f} dB")

    # Save
    save_dict = {
        'model_state': model.state_dict(),
        'latent1': latent1.detach().cpu(),
        'latent2': latent2.detach().cpu(),
        'config': {
            'hidden': hidden,
            'layers': layers,
            'latent_dim': latent_dim,
            'size': size,
        }
    }
    torch.save(save_dict, output_path)
    print(f"\nSaved: {output_path}")


def render_frame(model, coords, latent, size):
    """Render a single frame."""
    with torch.no_grad():
        output = model(coords, latent)
        img = output.reshape(size, size, 3).cpu().numpy()
        img = (img * 255).clip(0, 255).astype(np.uint8)
        return Image.fromarray(img)


def morph(model_path, output_path, duration=6, fps=30):
    """Generate smooth morph between the two learned images."""

    print(f"Loading model: {model_path}")
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)
    config = checkpoint['config']
    latent1 = checkpoint['latent1']
    latent2 = checkpoint['latent2']

    model = SIREN(
        hidden_features=config['hidden'],
        hidden_layers=config['layers'],
        latent_dim=config['latent_dim']
    )
    model.load_state_dict(checkpoint['model_state'])
    model.eval()

    size = config['size']
    coords = create_coordinate_grid(size, size)

    num_frames = duration * fps
    print(f"Rendering {num_frames} frames...")

    with tempfile.TemporaryDirectory() as tmpdir:
        for i in range(num_frames):
            t = i / num_frames
            # Smooth ping-pong: 0 -> 1 -> 0
            blend = 0.5 - 0.5 * math.cos(2 * math.pi * t)
            # Extra smoothstep
            blend = blend * blend * (3 - 2 * blend)

            # Interpolate in latent space
            latent = latent1 * (1 - blend) + latent2 * blend

            frame = render_frame(model, coords, latent, size)
            frame.save(f"{tmpdir}/frame_{i:04d}.png")

            if (i + 1) % fps == 0:
                print(f"  {i+1}/{num_frames} frames ({(i+1)//fps}s)")

        print("Encoding video...")
        subprocess.run([
            'ffmpeg', '-y', '-framerate', str(fps),
            '-i', f'{tmpdir}/frame_%04d.png',
            '-c:v', 'libx265', '-preset', 'medium',
            '-crf', '26', '-tag:v', 'hvc1',
            '-pix_fmt', 'yuv420p',
            output_path
        ], check=True, capture_output=True)

    size_kb = Path(output_path).stat().st_size / 1024
    print(f"Done! Output: {output_path} ({size_kb:.1f} KB)")


def main():
    parser = argparse.ArgumentParser(description='CPPN Multi-Image Learning')
    subparsers = parser.add_subparsers(dest='command', required=True)

    # Train command
    train_parser = subparsers.add_parser('train', help='Train on two images')
    train_parser.add_argument('image1', help='First image')
    train_parser.add_argument('image2', help='Second image')
    train_parser.add_argument('output', help='Output model (.pt)')
    train_parser.add_argument('--epochs', type=int, default=3000)
    train_parser.add_argument('--hidden', type=int, default=256)
    train_parser.add_argument('--layers', type=int, default=8)
    train_parser.add_argument('--latent-dim', type=int, default=32)

    # Morph command
    morph_parser = subparsers.add_parser('morph', help='Generate morph video')
    morph_parser.add_argument('model', help='Trained model (.pt)')
    morph_parser.add_argument('output', help='Output video (.mp4)')
    morph_parser.add_argument('--duration', type=int, default=6)
    morph_parser.add_argument('--fps', type=int, default=30)

    args = parser.parse_args()

    device = 'mps' if torch.backends.mps.is_available() else 'cpu'
    print(f"Using device: {device}")

    if args.command == 'train':
        train_multi(args.image1, args.image2, args.output, args.epochs,
                    args.hidden, args.layers, args.latent_dim, device=device)
    elif args.command == 'morph':
        morph(args.model, args.output, args.duration, args.fps)


if __name__ == '__main__':
    main()
